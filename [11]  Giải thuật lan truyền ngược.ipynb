{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài 11. Giải thuật lan truyền ngược"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong các phần trước, chúng ta đã tìm hiểu các thuật toán cơ bản đến nâng cao của **Neural Network**(nói nâng cao thì hơi quá, nhưng mà nâng cao này được cải tiến từ cơ bản). Tiếp tục, chúng ta sẽ tìm hiểu phần còn lại, đó là mạng neural nhiều tầng và giải thuật lan truyền ngược.\n",
    "\n",
    "Qua phần này(kèm theo các phần trước), chúng ta sẽ hiểu được\n",
    "- Hoạt động thực sự của một mạng nerual nhiều tầng.\n",
    "- Vấn đề xoay quanh hàm kích hoạt.\n",
    "- Giải thuật lan truyền ngược giúp tăng tốc thuật toán.\n",
    "- Hiểu được cái  thứ mà bạn nghĩ là mình sẽ không thể hiểu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nội dung\n",
    "1. Đặt vấn đề\n",
    "2. Mạng neural nhiều tầng\n",
    "3. Vấn đề về hàm kích hoạt\n",
    "4. Lan truyền ngược\n",
    "5. Demo Python\n",
    "6. Đánh giá MLP và Backpropagation\n",
    "7. Kết luận\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Đặt vấn đề"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dữ liệu của các thuật toán PLA, Logistic Regression, Softmax Regression là tách biệt nhau một cách rạch rồi(linearly seperable) hoặc gần như vậy. Vậy đối với bộ dữ liệu mà không có tính chất linearly seperable thì PLA, Logistic Regression SR không áp dụng vào được.\n",
    "\n",
    "![Non linearly seperable](https://scontent-hkg3-2.xx.fbcdn.net/v/t1.15752-9/72526872_2420766981364219_2092531916564594688_n.png?_nc_cat=111&_nc_oc=AQl9wS6fGRC1kBy6c8gZ6h7Yutp0aKfexNuPLbPGFwqQEDC8VRD6QnU-Um_VOhvEGK8&_nc_ht=scontent-hkg3-2.xx&oh=419d8e693bf781d986e29cc1010aee07&oe=5E201543)\n",
    "\n",
    "**Làm sau để phân lớp dataset không tính chất linearly seperable?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mạng Neural nhiều tầng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Slove problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lấy một bài toán cụ thể của bài toán mà yêu cầu đặt ra là phân chia tập dữ liệu của chúng ta với tính chất là non-linearly seperable thành các class một cách chính xấc như hình dưới đây.\n",
    "\n",
    "![](https://scontent.fhan5-5.fna.fbcdn.net/v/t1.15752-9/72408168_410686579849611_7077380847040462848_n.png?_nc_cat=101&_nc_oc=AQkVSF5gb_ifhPLdrx2kCSeuiM05SOSsaX-hUB9V-VYBgDD9QbSD18wFsAGl2XbyYVM&_nc_ht=scontent.fhan5-5.fna&oh=84516ee03896447ab8743b79349380eb&oe=5E2713D6)\n",
    "\n",
    "Hình trên trên mô phỏng bài toán xem một người với chiều cao H và cân nặng W có thuộc nhóm người bình thường hay không, hình vuông đại diện cho người không bình thường, trái lại hình tam giác đại diện cho người bình thường. Bạn có thể tìm được một đường thẳng mà phân chia rạch ròi hai lớp người(lớp người bình thường và lớp người không bình thường) hay không ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu trả lời là **không thể dùng một traight line mà phân hoạch rạch rồi 2 lớp dữ liệu này được** mà phải dùng nhiều hơn, tức là dùng 2 traight line để phân hoạch tập dữ liệu này như hình dưới đây."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scontent-hkg3-2.xx.fbcdn.net/v/t1.15752-9/72727537_250172749267595_3339821188468703232_n.png?_nc_cat=108&_nc_oc=AQmkmzJWhOdbejG1GgvBvCd2B1JykwuezcpBcUIXnbOyR_p11bVzVzwiniR2lVvv3Ss&_nc_ht=scontent-hkg3-2.xx&oh=c367ba5332a78638e42da91925c33b67&oe=5E23FC72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử hai đường thẳng mà ta tìm được để phân hoạch tập dữ liệu là $d_1$ và $d_2$, cùng với đó là quy ước dấu của các điểm dữ liệu đối với $d_1$ và $d_2$ như hình vẽ. Nếu một điểm dữ liệu nào mà rơi vào vùng có dấu $-$ đối với $d_1$ và dấu $+$ đối với $d_2$ thì điểm dữ liệu đó được phân vào lớp người bình thường, ngược lại nếu một điểm dữ liệu rơi vào vùng không phải vùng lý tưởng đó thì nó được phân vào lớp người không bình thường. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đến đây ta đã tìm ra được hướng giải quyết bài toán mà tập dữ liệu ko thỏa tính chất linearly seperable. Vấn đề còn lại là làm sao hiểu nó được giải quyết như nào, tức là tìm hai đường thẳng $d_1$ và $d_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiplayer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử hai đường thẳng mà ta tìm được trong bài toán trên là:\n",
    "- $d_1: x - y + 20 = 0$\n",
    "- $d_2: 2x - y - 10 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scontent.fsgn5-1.fna.fbcdn.net/v/t1.15752-9/73168108_960563354280663_2904838883688579072_n.jpg?_nc_cat=101&_nc_oc=AQnTRDW6XgKQHG5SbIEAfoqe93LDIhsIM9fYDUct3vvqlnb3nN7Ok0conZQEpVxG0a4&_nc_ht=scontent.fsgn5-1.fna&oh=9db757707de6e8abf3882a1498407c9a&oe=5E20F0AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bằng việc biễu diễn lại dưới dạng Neural network ta được như hình dưới đây:\n",
    "![](https://scontent.fsgn5-6.fna.fbcdn.net/v/t1.15752-9/72629684_409718349646053_8810240361013182464_n.jpg?_nc_cat=106&_nc_oc=AQl4JR9dULOamHwK8XUvExmjP43O2XvFZt6gemduiJsi9R-qgw0vvWOGdE8lVmMlg4c&_nc_ht=scontent.fsgn5-6.fna&oh=86f90097ef490ad1d6466bc9be262513&oe=5E63EAA8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tại sao ta có thể biểu diễn được như vậy(từ một graph sang một neural network) ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sử dụng 3 perceptron ở hình trên được xếp thành 2 layers(tầng). Layer thứ nhất: đầu vào-đỏ, đầu ra-xanh. Layer thứ hai, đầu vào màu xanh, đầu ra màu xanh. Ở đây đầu ra của layer thứ nhất chính là đầu vào của layer thứ hai. Tổng hợp lại ta được một mô hình mà ngoài layer đầu vào(đỏ) và đầu ra(xanh), ta còn một layer ở giữa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Một Neural network với nhiều hơn hai layer được gọi là Multilayer Neural Network**. Đôi khi còn được gọi là Deep feedforward network, feedforward neural network. **Từ Feedforward** được hiểu là dữ liệu đi thẳng từ đầu vào tới đầu ra theo các mũi tên mà không quay lại ở điểm nào, tức là network có dạng là một **cyclic graph(đồ thị không có chu trình kín)**. Trong ví du trên, ta xem xét hàm kích hoạt là hàm dấu $sgn$, nhưng thực tế ta thường dùng các hàm phi tuyến."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cụ thể hơn, một multilayer neural network là một neural network có nhiều layer, làm nhiệm vụ xấp xỉ mỗi quan hệ giữa các quan hệ $(x, y)$ trong tập huấn luyên bằng một hàm số có dạng:\n",
    "$$y \\approx g^{(L)}(g^{(L-1)}(...(g^{(2)}g^{(1)}(x)))))$$\n",
    "trong đó layer thứ nhất đống vai trò như hàm $g^{(1)}(x)$; layer thứ hai đóng vai trò như hàm $g^{(2)}(g^{(1)}))$\n",
    "\n",
    "Chúng ta quan tâm đến các layer đóng vai trò như các hàm có dạng:\n",
    "$$g^{(l)}(a(^{(l-1)})) = f^{(l)}(W^{(l)T})a^{(l-1)}+b^{(l)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/proxy/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg)\n",
    "\n",
    "Đầu ra của một Multilayer neural network loại này tương ứng với một đầu vào x có thể được tính theo:\n",
    "![](https://scontent.fsgn5-3.fna.fbcdn.net/v/t1.15752-9/72329531_1370509869797446_4761704304044146688_n.png?_nc_cat=111&_nc_oc=AQk3hDoNalUKI0Yw91NUAwqqQRUiu1zXCaOfl8Qlml1wro5Qbwl_WKhgLNaumi-NZzw&_nc_ht=scontent.fsgn5-3.fna&oh=9d74ce5bd0eb596a1e6b68bd5d933922&oe=5E32101A)\n",
    "\n",
    "Đây chính là đầu ra dự đoán. Bước này được gọi là feedforward vì cách tính toán được thực hiện từ đầu đến cuối của network. Hàm mất mát thỏa mãn đạt giá trị càng nhỏ khi đầu ra càng gần với đầu ra thực sự. Tùy vào bài toán, là classification hay là regression, chúng ta cần thiết kế các hàm mất mát thích hợp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Các kí hiệu và khái niệm liên quan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy ta đã biết được khái niệm Multilayer neural network, và nó được bắt nguồn từ bài toán phân lớp dữ liệu không có tính chất linearly seperable. Tiếp theo ta đi sâu vào các khái niệm trong Multilayer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer**: \n",
    "- Ngoài *input layer* và *output layer*, một multilayer neural  network có thể có nhiều hidden layer ở giữa. Các hidden layer theo thứ tự từ input layer đến output layer được đánh số thứ tự là hidden layer 1, hidden layer 2,... Hình dưới đây là một ví dụ về multilayer neural network với 2 hidden layer.\n",
    "![](https://scontent.fsgn5-4.fna.fbcdn.net/v/t1.15752-9/72362439_2142465482725057_3002025180324167680_n.png?_nc_cat=104&_nc_oc=AQmyT3RYr9q_gu7s3wHbxn6MjREf5ZPlw6hWZ7T-ABrHEnfJyW-5iFEe1D3sjIHyvps&_nc_ht=scontent.fsgn5-4.fna&oh=ca17ce99409e574f944cad0ea8fa0c0d&oe=5E1AB770)\n",
    "\n",
    "- Số lượng layer trong một multilayer neural network, được kí hiệu là L, được tính bằng số hidden layer cộng với 1. Khi đến số layer của một multilayer neural network ta không tính input layer. Trong hình trên, $L = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Units**:\n",
    "![](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the-chaotic.png)\n",
    "- Mỗi node hình tròn trong một layer được gọi là một unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights and biases**:\n",
    "- Có L ma trận trọng số cho một multilayer neural network có L layer.\n",
    "- Các ma trận này được ký hiệu là $W^{l}\\in R^{d(l-1)}\\times d^{(l)}, l = 1,2,...,L$, trong đó $W^{(l)}$ thể hiện các kết nối từ layer thứ $l-1$ đến layer thứ $l$(nếu ta coi input layer là layer thứ 0).\n",
    "- Cụ thể hơn nếu ta coi phần tử $w_{ij}^{(l)}$ thể hiện kết nối từ node thứ $i$ của layer thứ $(l-1)$ tới node thứ $j$ của layer thứ $l$.\n",
    "- Các bias của của layer thứ $(l)$ được kí hiệu là $b^{(l)} \\in R^{d(l)}$. \n",
    "- **Khi tối ưu một Multilayer neural network cho một công việc nào đó, chúng ta cần đi tìm các weight và bias này. Tập hợp các weight và bias này lần lượt kí hiệu là $W$ và $b$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vấn đề về hàm kích hoạt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Hàm kích hoạt là một hàm phi tuyến"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỗi output layer(trừ input layer) được tính dựa trên công thức:\n",
    "### $$a^{(l)} = f^{(l)}(W^{(l)T})a^{(l-1)}+b^{(l)})$$\n",
    "trong đó $f$ là một hàm kích hoạt phi tuyến. **Nếu hàm kích hoạt tại một layer là một hàm tuyến tính, layer này và layer tiếp theo có thể rút gọn thành một layer vì hợp của các hàm tuyến tính là một hàm tuyến tính** dẫn đến các layer không có ý nghĩa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hàm sgn không được dùng trong  MLP\n",
    "\n",
    "Hàm sgn chỉ sử dụng trong perceptron. Trong thực tế, hàm sgn không được sử dụng và đạo hàm tại hầu hết các điểm bằng 0(trừ tại điểm 0 không có đạo hàm). Việc đạo hàm bằng 0 khiến các thuật toán dựa trên Gradient Descent không hoạt động."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Sigmoid và tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sigmoid and tanh function](http://ronny.rest/media/blog/2017/2017_08_16_tanh/tanh_v_sigmoid.jpg)\n",
    "\n",
    "Hàm sigmoid có dạng $sigmoid(z) = 1/(1+exp(-z))$ với đồ thị như trên. Nếu giá trị đầu vào lớn, hàm số sẽ cho ra giá trị gần bằng 1, ngược lại càng nhỏ khi đầu vào có giá trị càng nhỏ. Hàm này trước đây được dùng nhiều vì có đạo hàm đẹp($\\sigma^{'}(x) = \\sigma(x)(1-\\sigma(x))$ ) nhưng gần đây thì ít được dùng.\n",
    "\n",
    "Hàm $tanh(z) = \\frac{exp(z)-exp(-z)}{exp(z)+exp(-z)}$ mang lại hiệu quả tốt hơn hàm sigmoid với miền giá trị là $[-1,1]$ khiến cho nó có tính chất zero-centered, thay vì dương hoàn toàn như hàm sigmiod.\n",
    "\n",
    "**Chú ý**: *Khi khởi tạo các hệ số cho Multilayer Neural Network với hàm kích hoạt là sigmoid, chúng ta phải tránh trường hợp đầu vào một hidden layer nào đó quá lớn, vì khi đó đầu ra của hidden layer đó sẽ rất gần với 0 hoặc 1, dẫn đến đạo hàm bằng 0 và GD không hoạt động được.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hàm ReLU\n",
    "\n",
    "**ReLU (Rectified Linear Unit)** được sử dụng rộng rãi trong những năm gần đây vì tính đơn giản của nó. Đồ thị của hàm ReLU được thể hiện dưới đây, với công thức: $f(z) = max(0, z)$\n",
    "![Sigmoid and ReLU function](https://miro.medium.com/max/1452/1*XxxiA0jJvPrHEJHD4z893g.png)\n",
    "\n",
    "Đạo hàm của nó bằng 0 tại các điểm âm, bằng 1 tại các điểm dương. **ReLU được chứng minh giúp cho việc huấn luyện Multilayer Neural Network hay Deep network nhanh hơn nhiều so với hàm `tanh`**.\n",
    "\n",
    "Mặc dù có nhược điểm là đạo hàm bằng 0 với các giá trị đầu vào âm nhưng ReLU được chứng minh bằng thực nghiệm rằng có thể khắc phục được việc này bằng cách tăng hidden layer unit. \n",
    "\n",
    "ReLU trở thành hàm kích hoạt đầu tiên chúng ta nên thử khi thiết kế một Multilayer neural network. Hầu hết các hidden layer đều dùng ReLU làm hàm kích hoạt trừ layer cuối cùng(output layer) có hàm kích hoạt phụ thuộc vào đầu ra yêu cầu của mỗi bài toán cụ thể.\n",
    "\n",
    "Ngoài ra còn có các biến thể của hàm ReLU:\n",
    "![Variant of ReLU function](http://www.programmersought.com/images/699/3ef07d67f4fb2e3085b92392e99cb69b.JPEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lan truyền ngược(backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phương pháp phổ biến nhất để tối ưu Multilayer neural network chính là Gradient Descent(GD). Để áp dụng GD chúng ta cần tính được đạo hàm của hàm mất mất theo từng ma trận trọng số $W$ và vector bias $b^{(l)}$.\n",
    "\n",
    "Giả sử $J(W, b, X, y)$ là một hàm mất mát của bài toán, trong đó $W, b$ là tập hợp tất cả các ma trận trọng số giữa các layer và vector bias của mỗi layer. $X, Y$ là cặp dữ liệu huấn luyện của mỗi cột tương ứng với mỗi điểm dữ liệu. Để có thể áp dụng được phương pháp GD, chúng ta cần tính được các $\\nabla _{W}J; \\nabla_{b^{(l)}}J, \\forall l = 1, 2, ..., L.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quá trình lan truyền thẳng(feedforward)**\n",
    "![Feedforward NN](https://scontent.fsgn5-3.fna.fbcdn.net/v/t1.15752-9/72329531_1370509869797446_4761704304044146688_n.png?_nc_cat=111&_nc_oc=AQk3hDoNalUKI0Yw91NUAwqqQRUiu1zXCaOfl8Qlml1wro5Qbwl_WKhgLNaumi-NZzw&_nc_ht=scontent.fsgn5-3.fna&oh=9d74ce5bd0eb596a1e6b68bd5d933922&oe=5E32101A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phương pháp được dùng phổ biến nhất được dùng có tên là backpropagation giúp tính đạo hàm ngược từ layer cuối cùng đến layer đầu tiên**. Layer cuối cùng được tính toán trước vì nó gần gũi hơn với đầu ra dự đoán và hàm mất mát. Việc tính toán đạo hàm của các ma trận hệ số trong các layer trước được thực hiện dựa trên quy tắc chuỗi quen thuộc cho các đạo hàm của hàm hợp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent(SGD) có thể được dùng để tính gradient cho các ma trận trọng số và biases dựa trên một cặp điểm training $x,y$. Để cho đơn giản, ta coi $J$ là hàm mất mát nếu chỉ xét cặp điểm này, ở đây J là hàm bất kì. Đạo hàm của hàm mất mát theo chỉ một thành phần của ma trận trọng số của output layer:\n",
    "![](https://scontent.fsgn5-3.fna.fbcdn.net/v/t1.15752-9/72706305_2402679016512546_399828310994976768_n.png?_nc_cat=110&_nc_oc=AQmGwMdFVitJnNbHOypFHZMM8nmw6o8PUcOq6ehZuIuABp1KcdDhwjNz8bXC--4wXTY&_nc_ht=scontent.fsgn5-3.fna&oh=cfb6f9a524f0911322087aa555e79668&oe=5E2B8D7C)\n",
    "Tương tự, đạo hàm của hàm mất mát theo bias của layer cuối cùng là:\n",
    "![](https://scontent.fsgn5-3.fna.fbcdn.net/v/t1.15752-9/72490258_456230731764300_5298085679966715904_n.png?_nc_cat=111&_nc_oc=AQlz3rCib0Nt52hhlMPvhXNRfQ118jmfCJkU31nr5OtEQ00VLvTwgXKsYBCj9B40aag&_nc_ht=scontent.fsgn5-3.fna&oh=4cb4a843aaef582270e8bfefbb3344c6&oe=5E192AAB)\n",
    "Dựa vào quy tắc chuỗi, ta tính được:\n",
    "![](https://scontent.fsgn5-6.fna.fbcdn.net/v/t1.15752-9/73313844_569929310414085_8079592163979034624_n.png?_nc_cat=106&_nc_oc=AQmslqeAzb0_F0MoZvDx-xBZaGbP5xp5HYR_KK-4gdr5lRzBhqw5rafQRhsPiw5N_-g&_nc_ht=scontent.fsgn5-6.fna&oh=30bf810e00a7363f573700b68139d779&oe=5E1A07A8)\n",
    "với\n",
    "![](https://scontent.fsgn5-5.fna.fbcdn.net/v/t1.15752-9/72703589_2406002362989320_4240453973351333888_n.png?_nc_cat=100&_nc_oc=AQmxP8SyrZfVxa5o_OYiqadOQ3M80U9mxIiO6fwWvkdjUfPc1hOTWNOWHXYXXyv4-2k&_nc_ht=scontent.fsgn5-5.fna&oh=85ab95ec64338b55ace705649ca45f8f&oe=5E5F668E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dấu $\\Sigma$ tính tổng ở dòng thứ hai trong phép tính trên xuất hiện vì $a_j$ đóng góp vào việc tính tất cả các $z_k^{(l+1)}, k =1,2,...,d^{(l+1)}$. Biểu thức đạo hàm ngoài dấu ngoặc lớn là vì $a_j^{l} = f^{(l)}$. Tới đây, ta có thể hiểu tại sao hàm kích hoạt có đạo hàm càng đơn giản thì sẽ càng có ích trong việc tính toán. Tương tự ta suy ra được:\n",
    "![](https://scontent.fsgn5-6.fna.fbcdn.net/v/t1.15752-9/72623640_418376238881097_7398802558656970752_n.png?_nc_cat=109&_nc_oc=AQnjl_yGUT_Dsy2U1IyJp5ZpTyqakEdR2BKKIrIefv_MPgz1r9rI5huNLgRyZaVNWg8&_nc_ht=scontent.fsgn5-6.fna&oh=984129fc24ac7ac7998cb2ef3b595e82&oe=5E63B0E1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận thấy rằng trong công thức trên đây, việc tính các $e_j^{(l)}$ đóng vai trò quan trọng. Hơn nữa, để tính giá trị này, ta cần tính được các $e_j^{(l+1)}$. Nói cách khác, ta cần tính ngược các giá trị này từ cuối. Cái tên backpropagation cũng xuất phát từ việc này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Đạo hàm theo từng hệ số $w_{ij}^{(l)}, b^{(l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scontent.fsgn5-6.fna.fbcdn.net/v/t1.15752-9/74418697_498518641066665_8632410478527643648_n.png?_nc_cat=109&_nc_oc=AQn94B5WAfu1KKv42xf4pWPzf4uFq2ul-qkUiS5Zy_YjwAwL9th7Gr09jSqN4tHbGI8&_nc_ht=scontent.fsgn5-6.fna&oh=84b23f0507474a9fc7f75805d241f7ee&oe=5E1B9289)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Đạo hàm theo ma trận $W^{(l)}, b^{(l)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scontent.fsgn5-1.fna.fbcdn.net/v/t1.15752-9/73423400_1317921828389005_4997175044233756672_n.png?_nc_cat=101&_nc_oc=AQko8tNyLZynxoRmaDRWr9oTlsEocEfY0XJVLgn4YmrHh2GOtcn_5YZFZvd0CGhc0qw&_nc_ht=scontent.fsgn5-1.fna&oh=20a8c4624ecc0ef6154e2ce3dece2f6f&oe=5E62A69C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Backpropagation cho batch(mini-batch) gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nếu chúng ta muốn thực hiện batch hoạt mini-batch GD thì thế nào ?**\n",
    "\n",
    "Trong thực tế, mini-batch GD được sử dụng nhiều nhất với các bài toán mà tập huấn luyện lớn. Nếu lượng dữ liệu là nhỏ, batch GD được dùng trực tiếp. Khi đó cặp (input, output) sẽ ở dạng ma trận $(X, y)$. Giả sử mỗi lần tính toán ta lấy N điểm dữ liệu để tính toán. Khi đó, $X \\in \\mathbb R^{d^{(0)}\\times N}, Y \\in \\mathbb R^{d^{(l)} \\times N} $.\n",
    "\n",
    "Khi đó các activation sau mỗi layer sẽ có dạng $A^{(l)} \\in \\mathbb R^{d^{(l)}\\times N}$. Tương tự, $E^{(l)} \\in \\mathbb R^{d^{(l)}\\times N}$. Và ta có công thức cập nhật như sau:\n",
    "\n",
    "![](https://scontent.fsgn5-5.fna.fbcdn.net/v/t1.15752-9/72397611_2329046057410259_971742483933298688_n.png?_nc_cat=108&_nc_oc=AQk4lHb0lJNzyyAHih8mSXs286vNyihnUoRgM9JEICr_9JW8uxw3KQiYjMkPszlMWt4&_nc_ht=scontent.fsgn5-5.fna&oh=92875468697c6359fcc36f4f45fc83c9&oe=5E1EA403)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demo trên Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mục này, chúng ta sẽ tạo dữ liệu giả trong không gian hai chiều sao cho đường ranh giới giữa các class không có dạng tuyến tính. Điều này khiến cho softmax regression không làm việc được. Tuy nhiên bằng cách thêm vào một hidden layer, chúng ta sẽ thấy rằng neural network này làm việc rất hiệu quả."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Tạo dữ liệu giả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100 # number of points per class\n",
    "d0 = 2 # dimensionality\n",
    "C = 3 # number of classes\n",
    "X = np.zeros((d0, N*C)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*C, dtype='uint8') # class labels\n",
    "\n",
    "for j in range(C):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*2,(j+1)*2,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[:,ix] = np.c_[r*np.sin(t), r*np.cos(t)].T\n",
    "  y[ix] = j\n",
    "# lets visualize the data:\n",
    "# plt.scatter(X[:N, 0], X[:N, 1], c=y[:N], s=40, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.plot(X[0, :N], X[1, :N], 'bs', markersize = 7);\n",
    "plt.plot(X[0, N:2*N], X[1, N:2*N], 'ro', markersize = 7);\n",
    "plt.plot(X[0, 2*N:], X[1, 2*N:], 'g^', markersize = 7);\n",
    "# plt.axis('off')\n",
    "plt.xlim([-1.5, 1.5])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_ticks([])\n",
    "cur_axes.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "plt.savefig('EX.png', bbox_inches='tight', dpi = 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Các hàm phụ trợ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(V):\n",
    "    e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
    "    Z = e_V / e_V.sum(axis = 0)\n",
    "    return Z\n",
    "\n",
    "## One-hot coding\n",
    "from scipy import sparse\n",
    "def convert_labels(y, C = 3):\n",
    "    Y = sparse.coo_matrix((np.ones_like(y),\n",
    "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
    "    return Y\n",
    "\n",
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return -np.sum(Y*np.log(Yhat))/Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Chương trình chính"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 1.098641\n",
      "iter 1000, loss: 0.063618\n",
      "iter 2000, loss: 0.048592\n",
      "iter 3000, loss: 0.041016\n",
      "iter 4000, loss: 0.035822\n",
      "iter 5000, loss: 0.032483\n",
      "iter 6000, loss: 0.030301\n",
      "iter 7000, loss: 0.028784\n",
      "iter 8000, loss: 0.027603\n",
      "iter 9000, loss: 0.026590\n"
     ]
    }
   ],
   "source": [
    "d0 = 2\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 3\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "Y = convert_labels(y, C)\n",
    "N = X.shape[1]\n",
    "eta = 1 # learning rate\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = softmax(Z2)\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(Y, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - Y )/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1[Z1 <= 0] = 0 # gradient of ReLU\n",
    "    dW1 = np.dot(X, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "\n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 99.00 %\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "print('training accuracy: %.2f %%' % (100*np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Đánh giá MLP và Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 [MLP xấp xĩ hầu hết các hàm liên tục](https://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Người ta đã chứng minh được rằng, với một hàm số liên tục bất kỳ $f(x)$ và một số $\\varepsilon >0$, luôn luôn tồn tại một Neural Network với predicted output có dạng $g(x)$ với một hidden layer (với số hidden units đủ lớn và nonlinear activation function phù hợp) sao cho với mọi $x, |f(x) -g(x)| < \\varepsilon$. Nói một cách khác, Neural Network có khả năng xấp xỉ hầu hết các hàm liên tục."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Khó khăn trong việc chọn số hidden units, hàm kích hoạt, Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trên thực tế, việc tìm ra số lượng hidden units và nonlinear activation function nói trên nhiều khi bất khả thi. Thay vào đó, thực nghiệm chứng minh rằng Neural Networks với nhiều hidden layers kết hợp với các nonlinear activation function (đơn giản như ReLU) có khả năng xấp xỉ (khả năng biểu diễn) training data tốt hơn.\n",
    "\n",
    "Khi số lượng hidden layers lớn lên, số lượng hệ số cần tối ưu cũng lớn lên và mô hình sẽ trở nên phức tạp. Sự phức tạp này ảnh hưởng tới hai khia cạnh. Thứ nhất, tốc độ tính toán sẽ bị chậm đi rất nhiều. Thứ hai, nếu mô hình quá phức tạp, nó có thể biểu diễn rất tốt training data, nhưng lại không biểu diễn tốt test data. Hay còn gọi hiện tượng này là **Overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Các mạng cải thiện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu mọi units của một layer được kết nối với mọi unit của layer tiếp theo (như chúng ta đang xét trong baì này), ta gọi đó là fully connected layer (kết nối hoàn toàn). Neural Networks với toàn fully connected layer ít được sử dụng trong thực tế. Thay vào đó, có nhiều phương pháp giúp làm giảm độ phức tạp của mô hình bằng cách giảm số lượng kết nối bằng cách cho nhiều kết nối bằng 0 (ví dụ, [sparse autoencoder](https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf)), hoặc các hệ số được ràng buộc giống nhau (để giảm số hệ số cần tối ưu) (ví dụ, [Convolutional Neural Networks (CNNs / ConvNets)](https://cs231n.github.io/convolutional-networks/)).\n",
    "\n",
    "Tham khảo thêm tại [đây](http://deeplearning.net/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết thúc phần này chúng ta biết cách khắc phục nhược điểm của các thuật toán Perceptron, Logistic Regression, Softmax Regression bằng cách sử dụng một mạng Neural Network nhiều tầng thay vì chỉ một tầng như ở các thuật toán trên. Một Neural Network có thể phân lớp tốt tập dữ liệu thành các lớp mà tập dữ liệu không có tính chất linearly seperable.\n",
    "\n",
    "Bản chất của Multilayer Neural Network vẫn là một hàm số, hàm số được kết hợp từ rất nhiều hàm số lại với nhau, được học dựa trên quá trình lan truyền ngược(Backpropagation).\n",
    "\n",
    "Kết thúc ANN tại đây, trong phần sau chúng ta sẽ tìm hiểu về một thuật toán khác trong bài toán phân lớp. Đó là Support Vector Machine(SVM) và có những đánh giá giữa SVM và ANN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
