{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 0208. Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp tục chuỗi bài toán trong Neural Network, trong phần này chúng ta tìm hiểu về Softmax Regression. Đây vẫn là một thuật toán áp dụng cho bài toán Multiable Classification. Dựa trên nền tảng của Logistic Regression, ta tiếp tục đi tối ưu hóa bài toán phân lớp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nội dung\n",
    "1. Mô hình One-vs-Rest\n",
    "2. Softmax Regression\n",
    "3. Hàm mất mát và phương pháp tối ưu\n",
    "4. Đánh giá Softmax Regression\n",
    "5. Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------##--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mô hình One-vs-Rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhắc lại, mô hình One-vs-Rest là mô hình dùng trong bài toán phân lớp đa lớp trong đó áp dụng thuật toán Logistic Regression. Ý tưởng của mô hình là đi xây dựng $C$ bộ phân lớp nhị phân mà mỗi bộ phân lớp nhị phân chỉ dùng để phân xem điểm dữ liệu đầu vào thuộc vào lớp này(one) hay không thuộc về lớp này, tức là thuộc về phần còn lại(rest).\n",
    "\n",
    "![One-vs-Rest](https://1.bp.blogspot.com/-cEz0wkXUPmA/WLJq3e6CDAI/AAAAAAAAFX4/Wramg-8qC_QH13K0qosnPAbvccXdbBU-ACLcB/s1600/VQvgVUs%255B1%255D.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Khi biểu diễn mô hình One-vs-Rest theo tư tưởng của Neural Network**:\n",
    "\n",
    "![One-vs-Rest_Neural_Network](https://scontent.fsgn1-1.fna.fbcdn.net/v/t1.15752-9/72103792_992367377766850_2292801650257035264_n.png?_nc_cat=108&_nc_eui2=AeGfR8kwX5JlS_jfQSo5lWFfwedsvtOannRph51K7JqVqWRO2SsikN4rd1f68imJXTHaPYsDQIKhU7SzyflQHveAUcy7YFjUDSI8GHfWL_Tdag&_nc_oc=AQmyq9qjNAp7DQ1XP3ayjIsIbldF6QPvf11QrvfkTQniw6jAubtO-DBgOWqsydGIgfA&_nc_ht=scontent.fsgn1-1.fna&oh=6ff21bd4ab2059d8c54321ee043a55cc&oe=5E39CAF7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với $a_i$ là một đại lượng trong đoạn $[0,1]$ thể hiện xác suất mà với vector $x_i$ đầu vào class $i$ nếu bộ tham số của mô hình là $w$.\n",
    "\n",
    "Một nhận xét đơn giản, với $C$ class:\n",
    "- $a_i \\in [0,1]$\n",
    "- $\\Sigma_{i=1}^C \\hspace{1mm}a_i = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Công thức Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xét hàm số:\n",
    "## $$a_i = \\frac{e^{z_i}}{\\Sigma_{j=1}^C e^{z_i}}, \\hspace{3mm} \\forall i = 1,2,3...$$\n",
    "\n",
    "Hàm số này, tính tất cả các $a_i$ dựa vào $z_i$, thõa mãn tất cả các điều kiện đã xét: dương, tổng bằng 1, giữ được thứ tự của $z_i$.\n",
    "\n",
    "**Hàm số này được gọi là `softmax function`**.\n",
    "\n",
    "Chú ý rằng với cách định nghĩa này, không có xác suất nào($a_i$ bất kì) có giá trị tuyệt đối bằng 0 hoặc bằng 1, mặc dù chúng có thể rất gần 0 hoặc 1 khi rất nhỏ hoặc rất lớn khi so sánh với các $z_j, j\\neq i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lúc này, ta có thể giả sử rằng:\n",
    "$$P(y_k=i|x_k,W) = a_i $$\n",
    "\n",
    "Trong đó, $P(y_k=i|x_k,W)$ được hiểu là xác suất để một điểm dữ liệu $x$ rơi vào class thứ $i$ nếu biết tham số mô hình (ma trận trọng số) là $W$. Mô hình Softmax Regression dưới con góc nhìn của Nearal Network:\n",
    "![Softmax_regression_with_NN](https://i.stack.imgur.com/0rewJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Softmax function trong Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in V.\n",
    "    each column of V is a set of score.    \n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z)\n",
    "    A = e_Z / e_Z.sum(axis = 0)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Một vài ví dụ\n",
    "\n",
    "![](https://scontent.fsgn1-1.fna.fbcdn.net/v/t1.15752-9/71646537_2320157064980756_7181849303676092416_n.png?_nc_cat=109&_nc_eui2=AeF4TV1nooF9xf4xlERp79vJjpLBgs89frgP1kG_aAjB62j34erm3aPV2xaFwtlTd9mOjv3Hgb68KuJeOvx6wMJ46MO1lZ7sdIudO7NfhA9puQ&_nc_oc=AQkT5zhBweS2YxCFWKPotxavHHr2ok3IxMgGJEhzWnYpLEVJfCDRBGXAdRUAxGUApmw&_nc_ht=scontent.fsgn1-1.fna&oh=2d9ce4bbfd6611d1b9cb32b8f0b7ee6e&oe=5E3182A0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có một vài quan sát như sau:\n",
    "- Cột 1: Nếu các $z_i$ bằng nhau, thì các $a_i$ cũng bằng nhau và bằng 1/3.\n",
    "- Cột 2: Nếu giá trị lớn nhất trong các $z_i$ là $z_1$ vẫn bằng 2, nhưng các giá trị khác thay đổi, thì mặc dù xác suất tương ứng $a_1$ vẫn là lớn nhất, nhưng nó đã thay đổi lên hơn 0.5. Đây chính là một lý do mà tên của hàm này có từ soft. (max vì phẩn từ lớn nhất vẫn là phần tử lớn nhất).\n",
    "- Cột 3: Khi các giá trị $z_i$ là âm thì các giá trị $a_i$ vẫn là dương và thứ tự vẫn được đảm bảo.\n",
    "- Cột 4: Nếu $z_1 = z_2 \\Rightarrow a_1 = a_2$.\n",
    "\n",
    "Đọc thêm tại: http://neuralnetworksanddeeplearning.com/chap3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Phiên bản ổn định hơn của softmax function\n",
    "\n",
    "Khi một trong các $z_i$ quá lớn, việc tính toán $e^{z_i}$ có thể gây ra hiện tượng tràn số (overflow), ảnh hưởng lớn tới kết quả của hàm softmax. Có một cách khắc phục hiện tượng này bằng cách dựa trên quan sát sau:\n",
    "$$\\frac{e^{z_i}}{\\Sigma_{j=1}^C \\hspace{1mm}} = \\frac{e^{-c}e^{z_i}}{e^{-c}\\Sigma_{j=1}^C \\hspace{1mm}^{z_j}} = \\frac{e^{z_i-c}}{\\Sigma_{j=1}^{C} \\hspace{1mm}e^{z_j-c}}$$\n",
    "\n",
    "với $c$ là một hằng số bất kì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đặt $f(z_i) =  \\frac{e^{z_i-c}}{\\Sigma_{j=1}^{C} \\hspace{1mm}e^{z_j-c}}$, thì $f(z_i)$ tốt hơn hàm $\\frac{e^{z_i}}{\\Sigma_{j=1}^C e^{z_i}}$ vì ta có thể chọn $c$ để cho việc tính toán thuận lợi hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in Z.\n",
    "    each column of Z is a set of score.    \n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 0)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Từ cách cài đặt, ta thấy chọn $c = max(Z)$, để giảm tối đa số lớn, vì vậy hàm này có tên là softmax**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hàm mất mát và phương pháp tối ưu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 One hot Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với cách biểu diễn network như trên, mỗi output sẽ không còn là một giá trị tương ứng với mỗi class nữa mà sẽ là một vector có đúng 1 phần tử bằng 1, các phần tử còn lại bằng 0. Phần tử bằng 1 năm ở vị trí tương ứng với class đó, thể hiện rằng điểm dữ liệu đang xét rơi vào class này với xác suất bằng 1 (sự thật là như thế, không cần dự đoán). Cách mã hóa output này chính là **one-hot coding**.\n",
    "\n",
    "Khi sử dụng mô hình Softmax Regression, với mỗi đầu vào $x$, ta sẽ có đầu ra dự đoán là $a = softmax(W^Tx)$. Trong khi đó, đầu ra thực sự chúng ta có là vector $y$ được biểu diễn dưới dạng one-hot coding.\n",
    "\n",
    "Hàm mất mát sẽ được xây dựng để tối thiểu sự khác nhau giữa đầu ra dự đoán $a$ và đầu ra thực sự $y$. Một lựa chọn đầu tiên ta có thể nghĩ tới là:\n",
    "\n",
    "$$J(W) = \\Sigma_{i=1}^N \\hspace{1mm}||a_i-y_i||_2^2$$\n",
    "\n",
    "Tuy nhiên đây chưa phải là một lựa chọn tốt. Khi đánh giá sự khác nhau (hay khoảng cách) giữa hai phân bố xác suất (probability distributions), chúng ta có một đại lượng đo đếm khác hiệu quả hơn. Đại lượng đó có tên là cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Xây dựng hàm mất mát"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tối ưu hàm mất mát"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Logistic Regression và Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi $C=2$, Softmax Regression và Logistic Regression là giống nhau. Thật vậy, đầu ra dự đoán của Softmax Regression với $C=2$ có thể được viết dưới dạng:\n",
    "\n",
    "$$a_1 = \\frac{e^{w_1^Tx}}{e^{w_1^Tx} + e^{w_2^Tx}} = \\frac{1}{1+e^{(w_2-w_1)^Tx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây chính là sigmoid function, là đầu ra dự đoán theo Logistic Regression. **Khi $C=2$, bạn cũng có thể thấy rằng hàm mất mát của Logistic và Softmax Regression đều là cross entropy**. Hơn nữa, mặc dù có 2 outputs, Softmax Regression có thể rút gọn thành 1 output vì tổng 2 outputs luôn luôn bằng 1. Softmax Regression còn có các tên gọi khác là Multinomial Logistic Regression, Maximum Entropy Classifier, hay rất nhiều tên khác nữa. \n",
    "\n",
    "Xem thêm [Multinomial logistic regression - Wikipedia](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Đánh giá Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Boundary tạo bởi Softmax Regression là linear*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Softmax Regression là một trong hai classifiers phổ biến nhất*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Regression** cùng với **Support Vector Machine** là hai classifier phổ biến nhất được dùng hiện nay. `Softmax Regression` đặc biệt được sử dụng nhiều trong các **mạng Neural có nhiều layer (Deep Neural Networks hay DNN).**\n",
    "\n",
    "Những lớp phía trước có thể được coi như một bộ Feature Extractor, lớp cuối cùng của DNN cho bài toán classification thường là Softmax Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này, chúng ta đã tìm hiểu thêm một thuật toán nữa trong con đường Neural Network, đó là Softmax Regression. Đây là một thuật toán có được nhờ quá trình cải tiến các thuật toán trước đó. Nhưng nó vẫn có vài điều chưa ổn ở một điểm nào đó trong bài toán nào đó. Vào phần sau chúng ta sẽ tiếp tục xem tiếp bài toán \"nào đó\", khuyết điểm \"nào đó\" được giải quyết nhờ những cải tiến tốt hơn nữa.\n",
    "\n",
    "![Back_Propagation](https://www.guru99.com/images/1/030819_0937_BackPropaga1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
