{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 0206. Perceptron Learning - Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này, chúng ta sẽ bàn về một thuật toán máy học xuất hiện đầu tiên trong lịch sử Machine  Learning, đó là Perceptron Learning.\n",
    "Thuật toán này xuất phát từ bài toán phân lớp (Classification). Thuật toán này là nền tảng để phát triển lên các thuật toán Neural Network, sau đó là Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nội dung\n",
    "1. Bài toán phân lớp nhị phân\n",
    "2. Thuật toán Perceptron\n",
    "3. Ví dụ minh họa\n",
    "4. Mô hình Neural Network đầu tiên\n",
    "5. Đánh giá Perceptron Learning\n",
    "6. Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------------------------------------##-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bài toán phân lớp nhị phân"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning là thuật toán từ tập từ liệu ban đầu đã được gán nhãn, đi xây dựng mô hình sau cho có thể dự đoán được output đầu ra ứng với điểm dữ liệu đầu vào. Classification là một thuật toán thuộc Supervised Learning, với output đầu ra là giá trị rời rạc, dùng để phân lớp đối tượng, sự vật, sự việc.\n",
    "\n",
    "![Classification Algorithm](https://miro.medium.com/max/1200/0*20-7V7NHXko2mizs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong đó, **bài toán phân lớp nhị phân** có thể được xem như việc xác định lãnh thổ của 2 class. Vì khi một điểm dữ liệu nào mà rơi vào lãnh thổ của class đó thì mặc nhiên được gán nhãn là nhãn là lớp đó. Từ đó ta đi tìm đường biên giới giữa hai  đất nước. Một đường biên giới như vậy sẽ giúp phân chia rạch ròi hai class(giả sử tồn tại). Một cách toán học, bài toán binary classification là tìm một đường thẳng(một mặt phẳng, một siêu phẳng) sau cho có thể phân biệt giữa hai class với nhau.\n",
    "\n",
    "![Binary Classification](https://waterprogramming.files.wordpress.com/2018/09/hyperplane1-e1538161721590.png?w=466&h=372)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron Learning Algorithm(PLA)** là một thuật toán đơn giản giúp tìm một siêu phẳng cho bài toán phân lớp nhị phân, giả sử tồn tại ranh giới phẳng đó. Nếu hai lớp dữ liệu đó có thể phân chia hoàn toàn bằng một siêu phẳng, ta nói hai lớp đó là \n",
    "**linearly seperable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Thuật toán Perceptron Leanring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Mô hình hóa bài toán*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thuật toán Supervising learning là thuật toán nhận input đầu vào là các điểm dữ liệu đã được gán nhãn(tập kinh nghiệp E), qua quá trình huấn luyện cho ra một mô hình có khả năng dự đoán output đầu ra với điểm dữ liệu đầu vào bất kì. Với bài toán phân lớp thì output của model là lớp mà điểm dữ liệu thuộc về. Đặc biểt, đối với binary classification thì chỉ có 2 lớp cần được phân. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử $X = [x_1, x_2, x_3,...,x_N] \\in R^{d \\times N}$ là ma trận chứa các điểm dữ liệu huấn luyện mà mỗi một cột trong $X$ là một vector đặc trưng trong không gian $d$ chiều. Giả sử thêm rằng các nhãn tương ứng với từng điểm dữ liệu được lưu trong một vector hàng $y = [y_1.y_2,y_3,...,y_N] \\in R^{1\\times N}$. Quy ước rằng: $y_i$ chỉ nhận một trong 2 giá trị là 1 hoặc -1, 1: thuộc về lớp thứ 1, -1: thuộc về lớp thứ 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tại một thời điểm, ta tìm được ranh giới giới chia 2 class là một siêu phẳng với phương trình:\n",
    "$$f_w(x) = w_1x_1 + ... + w_dx_d + w_0 = w^Tx + w0 = 0$$\n",
    ", bằng cách sử dụng bias trick ta được:\n",
    "$$f_w(x) = w^Tx = 0$$\n",
    ", với $x = [1,x_1,...,x_d]$ và $w = [w_0,w_1,...,w_d]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siêu phẳng này sẽ chia không gian $d$ chiều của chúng ta thành 2 phần. Giả sử ta quy ước 2 phần đó được phân biệt với nhau bằng dấu, khi đó ta có hàm xác định dấu\n",
    "\n",
    "$$label(x) = sign(w^Tx) = \n",
    "\\begin{align}\n",
    "    \\begin{cases} \n",
    "         1 \\hspace{2cm} if \\hspace{0.2cm}w^Tx > 0 \\\\\n",
    "        -1 \\hspace{2cm} o.w.\n",
    "    \\end{cases}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa vào hàm dấu này, chúng ta đi xây dựng hàm mất mát của thuật toán Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Xây dựng hàm mất mát*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài toán mà chúng ta đang đối phó đó là việc phân lớp một điểm dữ liệu mới, đưa ra output hoặc là -1, hoặc là 1. Từ đó, ta mong muốn việc dự đoán đúng lớp của điểm dữ liệu càng cao, suy ra việc đoán sai lớp của các điểm dữ liệu càng ít càng tốt. Hay nói cách khác, **Hàm mất mát của chúng ta chính là số lượng điểm dữ liệu bị phân sai lớp**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Khi nào thì một điểm dữ liệu bị liệu bị phân sai lớp ?\n",
    "**Một điểm dữ liệu bị phân sai lớp khi và chỉ khi output của model trái dấu với label thực sự của điểm dữ liệu đó**. Nói cách khác, nó đang ở nữa mặt phẳng dương mà lại dự đoán nó nằm ở mặt phẳng âm và ngược lại. Tức là:\n",
    "$$y_isgn(w^Tx_i) = -1$$. Như vậy hàm mất mát có công thức như sau:\n",
    "$$J_1(w) = \\sum_{x_i \\in M}(-y_isgn(w^Tx_i))$$\n",
    "trong đó $M$ là kí hiệu của tập hợp các điểm bị phân lớp sai ứng với $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hàm này là hàm rời rạc, nên khó khăn trong việc tối ưu. Vì vậy, chúng ta đưa hàm này về hàm liên tục bằng cách tính tổng số lỗi, và xem mỗi một lỗi có một gía trị để đánh giá chứ không dựa vào có là lỗi hay không.\n",
    "$$J(w) = \\sum_{x_i \\in M}(-y_iw^Tx_i)$$\n",
    "\n",
    "`Tại sao hàm rời rạc lại khó tối ưu ?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây chính là hàm mất mát sau cùng trong thuật toán PLA, nó được đánh là tốt hơn hẵn hàm trên nó vì nó phản ánh được mức độ lỗi, chứ không chỉ ra chung chung là có lỗi hay không có lỗi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Tối ưu hàm mất mát*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tối ưu hàm mất mát chính là đi tìm tham số của mô hình $w^*$ sao cho hàm mất mát đạt giá trị nhỏ nhất tại $w$. Có thể áp dụng các phương pháp tối ưu để giải nhưng ở đây ta sử dụng Gradient Descent hoặc Stochastic Gradient Descent. \n",
    "\n",
    "**Sử dụng Stochastic Gradient Descent bằng cách cập nhật w tại mỗi vòng lặp với chỉ duy nhất 1 điểm dữ liệu**.\n",
    "$$J(w_j;w_i;y_i) = -y_iw^Tx_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla _wJ(w;x_i;y_i) = -y_iwx_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", khi đó quy tắc cập nhật sẽ là:\n",
    "$$w \\leftarrow w - \\eta (-y_ix_i) = w + \\eta y_ix_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", với $\\eta$ là learning rate. Trong PLA, $\\eta$ được chọn là 1. Ta có quy tắc cập nhật rất gọn:\n",
    "$$w_{t+1} = w_t + y_ix_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Người ta chứng minh được rằng quá trình cập nhật nghiệm sẽ hội tụ sau một số hữu hạn bước.**\n",
    "\n",
    "*Chú ý: Việc chỉ dùng một điểm dữ liệu để cập nhật không đảm bảo hàm mất mát cho toàn bộ dữ liệu giảm, rất có thể biến một điểm dữ liệu nào đó đang ở trạng thái phân lớp đúng biến thành phân lớp sai.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ví dụ minh họa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giới thiệu bài toán phân loại chó ,mèo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài toán rất chi là đơn giản. Hãy huấn luyện một mô hình sao cho có thể phân biệt được chó hay mèo trong hình. Giả sử trong mỗi hình tồn tại duy nhất một con vật là chó hoặc mèo.\n",
    "\n",
    "Chúng ta sẽ dùng bộ dữ liệu từ cuộc thi Kaggle, với kích thước bộ dữ liệu là 125000 tấm ảnh chụp chó hoặc mèo. Bạn có thể download tại [đây](). Tải xuống và giải nén, lưu tất cả vào thư mục có tên là dataset có cấu trúc như sau.\n",
    "```\n",
    "...\n",
    "    dataset\\\n",
    "        train\\\n",
    "        \n",
    "        ....\n",
    "        test\\\n",
    "        \n",
    "        ....\n",
    "...\n",
    "```\n",
    "\n",
    "**Đương nhiên ta dùng các images trong folder dataset/train/ để training model, các images trong folder test dùng để test model.** Trước tiên hãy import các thư viện cần thiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Chuẩn bị dữ liệu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỗi một tấm ảnh dù là gì đi chăng nữa, thì máy tính hiểu nó như làm một ma trận 2 chiều. Từ một bức ảnh ta phải trích xuất được một vector đặc trưng, và vector đặc trưng đó sẽ là một hàng trong ma trận đầu vào của ta được lưu trong biến X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc du lieu khuon mat trong Dataset\n",
    "print(\"[INFO] loading images...\")\n",
    "path_train = \"Cat_Dog_dataset/train/\"\n",
    "path_test = \"Cat_Dog_dataset/test/\"\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "# Load dữ liệu training \n",
    "for i in range(1,1001):\n",
    "    # Đọc ảnh mèo\n",
    "    dir = path_train + \"cat.\" + str(i) + \".jpg\"\n",
    "    image = cv2.imread(dir)\n",
    "    y_train.append(1)\n",
    "    image = cv2.resize(image,(32,32)) \n",
    "    X_train.append(image)\n",
    "\n",
    "for i in range(1,1001):\n",
    "    # Đọc ảnh chó\n",
    "    dir = path_train + \"dog.\" + str(i) + \".jpg\"\n",
    "    image = cv2.imread(dir)\n",
    "    y_train.append(-1)\n",
    "    image = cv2.resize(image,(32,32)) \n",
    "    X_train.append(image)\n",
    "\n",
    "# Load dữ liệu test\n",
    "for i in range(1,601):\n",
    "    # Đọc ảnh mèo\n",
    "    dir = path_test + str(i) + \".jpg\"\n",
    "    image = cv2.imread(dir)\n",
    "    cv2.resize(image,(32,32))\n",
    "    X_test.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuan hoa du lieu anh ve [0,1]\n",
    "X_train = np.array(X_train, dtype=\"float\") / 255.0\n",
    "X_test = np.array(X_train, dtype=\"float\")/ 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Trích xuất các đặc trưng từ ảnh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Cài đặt thuật toán Perceptron Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như phân tích, ta cần có hàm xác định dấu và một hàm để thực thi thuật toán PLA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xác định dấu\n",
    "def predict(w, x):\n",
    "    \"\"\" Dự đoán nhãn của mỗi hàng dữ liệu với tham số w cho trước\"\"\"\n",
    "    return np.sign(X.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm cập nhật nghiệm\n",
    "def update(w, x, y):\n",
    "    \"\"\" Cập nhật lại nghiêm w với duy nhất một datapoint là (x,y)\"\"\"\n",
    "    w = w + y*x\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm thực thi thuật toán Perceptron Learning\n",
    "def Perceptron(X, y, w_init):\n",
    "    \"\"\" Diễn ra quá trình thuật toán Perceptron thực thi\n",
    "        Trong đó: - X là một ma trận gồm các vector đặc trưng, mỗi vector có kích thước là 1xd\n",
    "                    và X có kích thước là Nxd\n",
    "                  - y là label của N điểm dữ liệu được trích xuất thành N vector lưu trong X\n",
    "                  - w_init là giá trị khởi tạo cho bộ tham số của mô hình\n",
    "    \"\"\"\n",
    "    w = w_init\n",
    "    while True:\n",
    "        # Dự đoán \n",
    "        pred = predict(w,X)\n",
    "        # Tìm vị trí các điểm dữ liệu bị phân lớp lỗi\n",
    "        mis_idxs = np.where(np.equal(pred,y)==False)[0]\n",
    "        \n",
    "        # Số lượng các điểm bị phân lớp lỗi\n",
    "        num_mis = mis_idxs.shape[0]\n",
    "        # Nếu không còn điểm dữ liệu nào bị phân lớp sai, thì dừng\n",
    "        if num_idxs == 0:\n",
    "            return w\n",
    "        # Nếu còn điểm bị phân lớp lỗi thì chọn ngẫu nhiên một điểm dữ liệu \n",
    "        # trong tập hợp các điểm dữ liệu bị phân sai lớp\n",
    "        random_id = np.random.choice(mis_idxs, 1)[0]\n",
    "        \n",
    "        w = update(w,X[random_id],y[random_id])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Training model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "w_init = np.random.randn(X.shape[3])\n",
    "w = Perceptron(X, y, w_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Test Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mô hình Neural network đầu tiên"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The simple nearal network](https://scontent.fsgn5-7.fna.fbcdn.net/v/t1.15752-9/71644330_480980875824443_3533620794447888384_n.png?_nc_cat=103&_nc_oc=AQmof6TJBoc1jQX75UMOUZaI4snEhDTDdcDbIrwJa6s2opuBhyww6P9S-Lkm82QbcGI&_nc_ht=scontent.fsgn5-7.fna&oh=e1fb60df5a134f06f67865eebc81fa84&oe=5E2C1708)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm số dự đoán đầu ra của perceptron $label(x) = sgn(w^Tx)$ có thể được mô tả như hình trên. Đây chính là dạng đơn giản của Neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu vào của network được minh họa bằng các node màu lục với node $x_0$ luôn luôn bằng 1. Tập các node màu lục được gọi là **tầng đầu vào (input layer)**. Node $y = sgn(z)$ là output của network. Ký hiệu chữ Z ngược màu làm lam trong node y thể hiện đồ thị hàm sgn. Dữ liệu đầu vào được đặt vào input layer, lấy tổng trọng số lưu vào biến z rồi đi qua hàm kich hoạt để có kết quả y. Đây chính là dạng đơn giản nhất của một neural network. Perceptron được vẽ đơn giản như hình b.\n",
    "\n",
    "Các neural network có thể có nhiều node ở output tạo thành tầng đầu ra(output layer), hoặc có thể có thêm các layer trung gian giữa input layer và output layer, các tầng như vậy được gọi là tầng ẩn(hidden layer).\n",
    "\n",
    "Perceptron đơn giản không có hidden layer. Để ý rằng nếu thay activation function bởi hàm identity $y = z$, ta sẽ có **network mô tả linear regreession** như hình c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network với Hidden Layers](https://miro.medium.com/proxy/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Đánh giá Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLA có thể cho vô số nghiệm khác nhau miễn là đường thẳng đó có đó phân biệt rõ ràng 2 class**. Vậy chọn đường thẳng nào trong các đường thẳng này? **Đường thẳng nào là tốt nhất?** \n",
    "\n",
    "![Distince of PLA](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/images/svm/margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLA đòi hỏi hai tập lớp phải phân biệt với nhau tuyệt đối(linearly seperable).** Nếu có xuất hiện nhiễu nằm lẫn trong 2 lớp thì PLA sẽ không thể thực hiện được, tức không bao giờ dừng lại, trong trường hợp này vì với mội đương thẳng ranh giới, luôn có ít nhất hai điểm bị phân lớp lỗi.\n",
    "\n",
    "![2. The distince of PLA](https://machinelearningcoban.com/assets/pla/pla7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pocket Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong một chừng mực nào đó, đường thẳng màu đen vẫn có thể là một nghiệm tốt vì nó làm việc hầu như chính xác. việc không hội tụ với dữ liệu gần linearly seperable chính là nhược điểm lớn của PLA.\n",
    "\n",
    "Nhược điểm này được khắc phục bằng thuật bằng pokect algorithm dưới đây."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pocket Algorithm**: một cách tự nhiên, nếu có một vài nhiễu, ta sẽ đi tìm một đường thẳng phân chia 2 class sao cho có ít điểm bị phân lỗi nhất. Việc này có thể được thức hiện thông qua PLA với một chút thay đổi nhỏ:\n",
    "\n",
    "1. Giới hạn số lượng vòng lặp của PLA. Đặt nghiệm $w$ sau vòng lặp đầu tiên là số điểm bị phân lớp lỗi vào trong túi quần(pocket).\n",
    "2. Mỗi lần lặp cập nhật nghiệm $w_t$ mới, ta đếm xem có bao nhiêu điểm bị phân lớp lỗi. So sánh số điểm bị phân lớp lỗi này với số điểm bị phân lớp lỗi trong pokect, nếu nhỏ hơn thì lấy nghiệm cũ ra, đặt nghiệm mới vào. Lặp lại bước này đến khi hết số vòng lặp.\n",
    "\n",
    "*Thuật toán Pocket Algorithm được hình dung như việc tìm phân tử nhỏ nhất trong mảng một chiều bằng kĩ thuật đặt lính canh.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qua phần này, chúng ta đã tìm hiểu được một thuật toán mở đầu cho Neural Network, Perceptron Learning. Thuật toán này giải quyết bài toán Binary Classification. Thuật toán này vẫn còn nhiều hạn chế như trên, và thực tế thì không được áp dụng nhiều. Logistic Regression là thuật toán dựa trên tư tưởng của Perceptron Learning nhưng có tính đột phá và được dùng nhiều, sẽ được giới thiệu trong phần sau."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
