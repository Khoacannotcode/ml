{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài 3. K-nestest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở phần trước chúng ta đã làm quen với thuật toán Linear Regression(Hồi quy tuyến tính), xây dựng mô hình để dự đoán output cho dữ liệu đầu vào, và output ở đây là giá trị liên tục. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thuật toán k-NN được cho là thuật toán học máy đơn giản nhất. Xây dựng mô hình chỉ bao gồm lưu trữ tập dữ liệu huấn luyện. Để đưa ra dự đoán cho một điểm dữ liệu mới, thuật toán tìm các điểm dữ liệu gần nhất trong tập dữ liệu huấn luyện hay chính là láng giềng gần nhất của nó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K-nestest Neighbords](https://miro.medium.com/max/560/1*Ug2_0J2UhylXx0Jx6TbRzg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nội dung\n",
    "1. Tư tưởng của K-nestest Neighbords\n",
    "2. Phân tích toán học\n",
    "3. Ví dụ trên Python\n",
    "4. Đánh giá k-NN\n",
    "5. Kết luận\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tư tưởng của K  - nestest Neighbords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Ví dụ dẫn nhập*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một ngày nọ, bạn đi thi môn Toán Thống kê, bạn chưa ôn bài, nhưng bạn tự tin bạn có rất nhiều tài liệu, bài tập đã giải có thể mang vào phòng thi(đề mở). Bạn khá lười, và nghĩ rằng một đống tài liệu đó, đâu đó sẽ có nhưng câu hỏi mà trong đề thi có thể xuất hiện, hoặc là các câu hỏi trong đề thi có thể xuất hiện sẽ gần giống câu hỏi nào đó trong xấp tài liệu bạn mang theo. Từ cơ sở đó, bạn có thể chọn đáp án cho các câu hỏi trong bài thi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy hình dung qúa trình bạn thi như sau:\n",
    "- Bạn có rất nhiều tài liệu có sẵn, giả sử là một **bộ câu hỏi đã có đáp án**.\n",
    "- Bạn **trả lời từng câu hỏi** bằng cách chọn đáp án dựa trên **câu hỏi gần giống với câu hỏi đã trong bộ câu hỏi ôn tập**. Tức trong bộ câu hỏi sẽ có **những câu gần với câu hỏi thực sự bạn đang làm**, bạn *dựa vào những câu hỏi đã có đáp án này để đưa ra đáp án cho câu hỏi bài thi*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *KNN là gì?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thực sự rất đơn giản trong việc đưa ra đáp án với cơ sở này. Hãy tưởng tượng bạn là một máy tính, thuật toán KNN có tư tương giống như quá trình bạn học một cách thụ động như vậy. \n",
    "![](https://pics.me.me/caption-by-kittyworlds-im-not-lazy-im-just-very-relaxed-21068308.png)\n",
    "Một đặc điểm của thuật toán K-nestest Neighbords đó máy tính không học được bất kỳ từ tập dữ liệu cả. Nó chỉ học khi nó bắt đầu testing. Vì vậy người ta gọi KNN là một thuật toán **lazy learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mọi tính toán được thực hiện khi nó cần dự đoán kết quả của dữ liệu mới. **K-nearest neighbor có thể áp dụng được vào cả hai loại của bài toán Supervised learning là Classification và Regression**. KNN còn được gọi là một thuật toán Instance-based hay Memory-based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Với KNN, trong bài toán Classification, label của một điểm dữ liệu mới (hay kết quả của câu hỏi trong bài thi) được suy ra trực tiếp từ K điểm dữ liệu gần nhất trong training set. Label của một test data có thể được quyết định bằng major voting (bầu chọn theo số phiếu) giữa các điểm gần nhất, hoặc nó có thể được suy ra bằng cách đánh trọng số khác nhau cho mỗi trong các điểm gần nhất đó rồi suy ra label.\n",
    "![](http://drive.kenh9.tv/http/1200x1200/tinhhoa.net-U9zp8B-20151022-tai-sao-con-nguoi-lai-hay-bat-chuoc-theo-dam-dong.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong bài toán Regresssion, đầu ra của một điểm dữ liệu sẽ bằng chính đầu ra của điểm dữ liệu đã biết gần nhất (trong trường hợp K=1), hoặc là trung bình có trọng số của đầu ra của những điểm gần nhất, hoặc bằng một mối quan hệ dựa trên khoảng cách tới các điểm gần nhất đó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Một cách ngắn gọn, KNN là thuật toán đi tìm đầu ra của một điểm dữ liệu mới bằng cách chỉ dựa trên thông tin của K điểm dữ liệu trong training set gần nó nhất (K-lân cận), không quan tâm đến việc có một vài điểm dữ liệu trong những điểm gần nhất này là nhiễu*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phân tích toán học"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Nhắc lại khái niệm khoảng cách*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Làm sao biết được điểm dữ liệu gần nhất với điểm dữ liệu đang xét? Dựa vào khoảng cách? Khoảng cách được tính như thế nào? Liệu cách tính khoảng cách bạn đưa ra có hợp ý trong ngữ cảnh này?*\n",
    "![](https://www.memecreator.org/static/images/memes/5102972.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuẩn khoảng cách thường được dùng nhiều nhất trong việc tính khoảng cách giữa 2 điểm dữ liệu(2 vector) là khoảng cách Euclid hay là chuẩn $l_2$ norm. Khoảng cách giữa 2 điểm dữ liệu là đại lượng biểu hiện sự sai khác giữa 2 điểm dữ liệu. Tức là, **2 điểm dữ dữ liêu càng giống nhau khi và chỉ khi khoảng cách giữa 2 điểm dữ liệu này càng nhỏ**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Công thức tính khoảng cách giữa 2 điểm dữ liệu theo chuẩn $l_2$ hay khoảng cách Euclid:\n",
    "$$l_2 = AB = \\sqrt{(x_1 - y_1)^2 + (x_2-y_2)^2+(x_3-y_3)^2+...+(x_n-y_n)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Mô hình hóa bài toán*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử ta có $N$ điểm dữ liệu đã đươc gán nhãn sẵn. Nhiệm vụ của ta là từ training data có sẵn đó đi xây dựng một model có thể cho ra output với điểm dữ liệu input bất kỳ, sao cho output này giống với label thực tế của dữ liệu kiểm thử nhất. Đây là bài tóan **Supervied Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đối với bài toán con KNN, từ tập dữ liệu đã được gán nhãn, mỗi khi có một điểm dữ liệu mới ta sẽ tìm output tương ứng cho nó bằng cách tính khoảng cách từ điểm dữ liệu mới này đến tất cả điểm dữ liệu có sẵn, từ đó chọn ra K điểm dữ liệu gần nhất với nó. Từ tỉ lệ phiếu bầu và label của K điểm dữ liệu ta suy ra label của điểm dữ liệu kiểm thử.**\n",
    "![](https://cambridgecoding.files.wordpress.com/2016/01/knn2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ví dụ trên Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trăm nghe không bằng một thấy. Chúng ta sẽ thực hành một ví dụ để hiểu rõ quá trình xây dựng model như thế nào. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Bộ cơ sở dữ liệu Iris (Iris flower dataset).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) là một bộ dữ liệu nhỏ (nhỏ hơn rất nhiều so với [MNIST](https://machinelearningcoban.com/2017/01/04/kmeans2/#bo-co-so-du-lieu-mnist). Bộ dữ liệu này bao gồm thông tin của ba loại hoa Iris (một loài hoa lan) khác nhau: Iris setosa, Iris virginica và Iris versicolor. Mỗi loại có 50 bông hoa được đo với dữ liệu là 4 thông tin: chiều dài, chiều rộng đài hoa (sepal), và chiều dài, chiều rộng cánh hoa (petal). Dưới đây là ví dụ về hình ảnh của ba loại hoa. (Chú ý, đây không phải là bộ cơ sở dữ liệu ảnh như MNIST, mỗi điểm dữ liệu trong tập này chỉ là một vector 4 chiều).\n",
    "![](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Thí nghiệm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này, chúng ta sẽ **tách 150 dữ liệu trong Iris flower dataset ra thành 2 phần**, gọi là `training set` và `test set`. Thuật toán KNN sẽ dựa vào trông tin ở training set để dự đoán xem mỗi dữ liệu trong test set tương ứng với loại hoa nào. Dữ liệu được dự đoán này sẽ được đối chiếu với loại hoa thật của mỗi dữ liệu trong test set để đánh giá hiệu quả của KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trước tiên, chúng ta cần khai báo vài thư viện.\n",
    "\n",
    "Iris flower dataset có sẵn trong thư viện [scikit-learn](http://scikit-learn.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiếp theo, chúng ta load dữ liệu và hiện thị vài dữ liệu mẫu. Các class được gán nhãn là 0, 1, và 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n",
      "Number of data points: 150\n",
      "\n",
      "Samples from class 0:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "Samples from class 1:\n",
      " [[7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]]\n",
      "\n",
      "Samples from class 2:\n",
      " [[6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "print('Number of classes:', len(np.unique(iris_y)))\n",
    "print('Number of data points:',len(iris_y))\n",
    "\n",
    "\n",
    "# Các điểm dữ liệu có nhãn là 0\n",
    "X0 = iris_X[iris_y == 0,:]\n",
    "print('\\nSamples from class 0:\\n', X0[:5,:])\n",
    "\n",
    "# Các điểm dữ liệu có nhãn là 1\n",
    "X1 = iris_X[iris_y == 1,:]\n",
    "print('\\nSamples from class 1:\\n', X1[:5,:])\n",
    "\n",
    "# Các điểm dữ liệu có nhãn là 2\n",
    "X2 = iris_X[iris_y == 2,:]\n",
    "print('\\nSamples from class 2:\\n', X2[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Tách training và test sets*\n",
    "Giả sử chúng ta muốn dùng 50 điểm dữ liệu cho test set, 100 điểm còn lại cho training set. Scikit-learn có một hàm số cho phép chúng ta ngẫu nhiên lựa chọn các điểm này, như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  100\n",
      "Test size  50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     iris_X, iris_y, test_size=50)\n",
    "\n",
    "print(\"Training size: \", len(y_train))\n",
    "print(\"Test size \", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sau đây, chúng ta trước hết xét trường hợp đơn giản K = 1, tức là với mỗi điểm test data, ta chỉ xét 1 điểm training data gần nhất và lấy label của điểm đó để dự đoán cho điểm test này.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print results for 20 test data points:\n",
      "Predicted labels:  [2 1 0 1 2 1 2 0 0 1 2 0 0 2 1 0 2 2 0 0]\n",
      "Ground truth    :  [2 1 0 1 2 1 2 0 0 1 2 0 0 1 1 0 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "clf = neighbors.KNeighborsClassifier(n_neighbors = 1, p = 2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Print results for 20 test data points:\")\n",
    "print(\"Predicted labels: \", y_pred[20:40])\n",
    "print(\"Ground truth    : \", y_test[20:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả cho thấy label dự đoán gần giống với label thật của test data, chỉ có 2 điểm trong số 20 điểm được hiển thị có kết quả sai lệch. Ở đây chúng ta làm quen với khái niệm mới: **ground truth**. Một cách đơn giản, **ground truth chính là nhãn/label/đầu ra thực sự của các điểm trong test data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Phương pháp đánh giá (evaluation method)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để đánh giá độ chính xác của thuật toán KNN classifier này, chúng ta xem xem có bao nhiêu điểm trong test data được dự đoán đúng. Lấy số lượng này chia cho tổng số lượng trong tập test data sẽ ra độ chính xác."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1NN:  94.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy of 1NN: \", (100*accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chú ý rằng đây là một cơ sở dữ liệu dễ, chúng ta đã có thể suy ra quy luật. Trong ví dụ này, chúng ta sử dụng **`p = 2` nghĩa là khoảng cách ở đây được tính là khoảng cách theo norm 2 (khoảng cách Eucild)**. Các bạn cũng có thể thử bằng cách thay p = 1 cho norm 1, hoặc các gía trị p khác cho norm khác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận thấy rằng chỉ xét 1 điểm gần nhất có thể dẫn đến kết quả sai nếu điểm đó là nhiễu. Một cách có thể làm tăng độ chính xác là tăng số lượng điểm lân cận lên, ví dụ 10 điểm, và xem xem trong 10 điểm gần nhất, class nào chiếm đa số thì dự đoán kết quả là class đó. Kỹ thuật dựa vào đa số này được gọi là **major voting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neighbors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-54b5cc112002>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy of 10NN with major voting:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'neighbors' is not defined"
     ]
    }
   ],
   "source": [
    "clf = neighbors.KNeighborsClassifier(n_neighbors=10, p=2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of 10NN with major voting:\", (100*accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Đánh trọng số cho các điểm lân cận*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta chưa muốn dừng kết quả ở đây vì thấy rằng mình vẫn có thể cải thiện được. **Trong kỹ thuật major voting bên trên, mỗi trong 10 điểm gần nhất được coi là có vai trò như nhau và giá trị lá phiếu của mỗi điểm này là như nhau**. Liệu như vậy đã công bằng, vì rõ ràng rằng những điểm gần hơn nên có trọng số cao hơn (`càng thân cận thì càng tin tưởng`). Vậy nên chúng ta sẽ đánh trọng số khác nhau cho mỗi trong 10 điểm gần nhất này. Cách đánh trọng số phải thoải mãn điều kiện là một điểm càng gần điểm test data thì phải được đánh trọng số càng cao (tin tưởng hơn). Cách đơn giản nhất là lấy nghịch đảo của khoảng cách này. (Trong trường hợp test data trùng với 1 điểm dữ liệu trong training data, tức khoảng cách bằng 0, ta lấy luôn label của điểm training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10NN (1/distance weights): 96.0\n"
     ]
    }
   ],
   "source": [
    "clf = neighbors.KNeighborsClassifier(n_neighbors=10, p=2, weights='distance')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of 10NN (1/distance weights):\", (100*accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chú ý: Ngoài 2 phương pháp đánh trọng số `weights = 'uniform'` và `weights = 'distance'` ở trên, `scikit-learn` còn cung cấp cho chúng ta một cách để đánh trọng số một cách tùy chọn. Ví dụ, một cách đánh trọng số phổ biến khác trong Machine Learning là:\n",
    "$$w_i = e^{\\frac{-||x-x_i||_2^2}{\\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trong đó là test data, là một điểm trong K-lân cận của là trọng số của điểm đó (ứng với điểm dữ liệu đang xét là một số dương. Nhận thấy rằng hàm số này cũng thỏa mãn điều kiện: điểm càng gần thì trọng số càng cao (cao nhất bằng 1). Với hàm số này, chúng ta có thể lập trình như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10NN (customized weights): 92.0\n"
     ]
    }
   ],
   "source": [
    "def myweight(distances):\n",
    "    sigma2 = .5 # we can change this number\n",
    "    return np.exp(-distances**2/sigma2)\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors = 10, p = 2, weights = myweight)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of 10NN (customized weights):\", (100*accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Đánh giá k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Ưu điểm của KNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Độ phức tạp tính toán của quá trình training là bằng 0**.\n",
    "- Việc dự đoán kết quả của dữ liệu mới rất đơn giản.\n",
    "- Không cần giả sử gì về phân phối của các class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Khuyết điểm KNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **KNN rất nhạy cảm với nhiễu khi K nhỏ**.\n",
    "- Như đã nói, KNN là một thuật toán mà mọi tính toán đều nằm ở quá trình test. Trong đó việc tính khoảng cách tới từng điểm dữ liệu trong training set sẽ tốn rất nhiều thời gian, đặc biệt là với các cơ sở dữ liệu có số chiều lớn và có nhiều điểm dữ liệu. Với K càng lớn thì độ phức tạp cũng sẽ tăng lên. Ngoài ra, việc lưu toàn bộ dữ liệu trong bộ nhớ cũng ảnh hưởng tới hiệu năng của KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Tăng tốc cho KNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngoài việc tính toán khoảng cách từ một điểm test data đến tất cả các điểm trong traing set (Brute Force), có một số thuật toán khác giúp tăng tốc việc tìm kiếm này. Bạn có thể tìm hiểu chúng qua các từ khóa: [K-D Tree](http://pointclouds.org/documentation/tutorials/kdtree_search.php) và [Ball Tree](https://en.wikipedia.org/wiki/Ball_tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đọc thêm về bộ dữ liệu [MNIST](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này, chúng ta tìm hiểu thêm một thuật toán có tên k-Nestest Neighbords, sở dĩ nó có tên như vậy vì quá trình gán nhãn cho điểm dữ liệu mới dựa trên khoảng cách đến các người láng giềng gần nhất của nó. Trong phần sau, chúng ta sẽ tìm hiểu về một thuật toán dựa trên lý thuyết xác suất, Naive Bayes Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Tài liệu tham khảo*\n",
    "[sklearn.neighbors.NearestNeighbors](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors)\n",
    "\n",
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "[Tutorial To Implement k-Nearest Neighbors in Python From Scratch](http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Chú ý: phần `Ví dụ Python` chỉ thể hiện cho thuật toán Classification, bạn có thể áp dụng cách này cho bài toán Regression, cũng dựa vào khoảng cách của K láng giềng gần nhất để đưa giá trị dự đoán hồi quy.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
